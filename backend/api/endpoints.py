#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIé©±åŠ¨çš„æ•°å­¦å¯è§†åŒ–APIç«¯ç‚¹
"""

import uuid
import time
import asyncio
from datetime import datetime
from typing import Dict, Any, Optional
from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends
from fastapi.responses import JSONResponse

from backend.models.schema import (
    ProblemRequest, TaskResponse, TaskInfo, HealthStatus,
    LLMProvider, ProcessingMode, TaskStatus,
    AIAnalysisResult, ExecutionResult
)
from backend.ai_service import get_code_generator, get_llm_manager
from backend.execution import get_sandbox_manager
from backend.config import get_config_manager

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/api/v2", tags=["AI Math Visualization"])

# ä»»åŠ¡å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨Redisæˆ–æ•°æ®åº“ï¼‰
task_storage: Dict[str, TaskInfo] = {}

# é…ç½®å­˜å‚¨
config_storage = {
    "api_keys": {},
    "default_provider": LLMProvider.OPENAI,
    "default_execution_mode": "restricted",
    "max_concurrent_tasks": 10
}

async def process_ai_visualization_task(task_id: str, request: ProblemRequest):
    """
    å¤„ç†AIå¯è§†åŒ–ä»»åŠ¡çš„åå°å‡½æ•°
    
    Args:
        task_id: ä»»åŠ¡ID
        request: è¯·æ±‚å‚æ•°
    """
    try:
        print(f"ğŸš€ [TASK-{task_id}] å¼€å§‹å¤„ç†AIå¯è§†åŒ–ä»»åŠ¡")
        print(f"ğŸ“ [TASK-{task_id}] é¢˜ç›®å†…å®¹: {request.text}")
        print(f"ğŸ”§ [TASK-{task_id}] LLMæä¾›å•†: {request.llm_provider}")
        print(f"ğŸ“‹ [TASK-{task_id}] æ¨¡æ¿å˜ä½“: {request.prompt_variant}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼šå¼€å§‹AIåˆ†æ
        update_task_status(task_id, TaskStatus.AI_ANALYZING, 10)
        print(f"ğŸ“Š [TASK-{task_id}] çŠ¶æ€æ›´æ–°: AI_ANALYZING (10%)")
        
        # 1. è°ƒç”¨AIç”Ÿæˆä»£ç 
        print(f"ğŸ¤– [TASK-{task_id}] å¼€å§‹è°ƒç”¨LLMç”Ÿæˆä»£ç ...")
        code_generator = get_code_generator()
        
        try:
            ai_result = await code_generator.generate_visualization_code(
                problem_text=request.text,
                output_path=f"output/{task_id}.png",
                provider=request.llm_provider,
                template_variant=request.prompt_variant
            )
            
            print(f"âœ… [TASK-{task_id}] LLMä»£ç ç”Ÿæˆå®Œæˆ!")
            print(f"ğŸ“ˆ [TASK-{task_id}] ç½®ä¿¡åº¦: {ai_result.confidence:.2f}")
            print(f"â±ï¸  [TASK-{task_id}] å¤„ç†æ—¶é—´: {ai_result.processing_time:.2f}ç§’")
            print(f"ğŸ·ï¸  [TASK-{task_id}] é¢˜ç›®ç±»å‹: {ai_result.problem_type}")
            print(f"ğŸ“ [TASK-{task_id}] å‚æ•°: {ai_result.parameters}")
            print(f"ğŸ’¬ [TASK-{task_id}] è¯´æ˜: {ai_result.explanation[:100]}...")
            print(f"ğŸ“ [TASK-{task_id}] ä»£ç é•¿åº¦: {len(ai_result.visualization_code)} å­—ç¬¦")
            
            # æ·»åŠ è¯¦ç»†çš„LLMäº¤äº’è°ƒè¯•ä¿¡æ¯
            if hasattr(ai_result, 'llm_interaction') and ai_result.llm_interaction:
                llm_interaction = ai_result.llm_interaction
                print(f"ğŸ” [TASK-{task_id}] LLMäº¤äº’è¯¦æƒ…:")
                print(f"   å“åº”æ—¶é—´: {llm_interaction.response_time:.2f}ç§’")
                print(f"   ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(llm_interaction.system_prompt)} å­—ç¬¦")
                print(f"   ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(llm_interaction.user_prompt)} å­—ç¬¦")
                print(f"   LLMå“åº”é•¿åº¦: {len(llm_interaction.response_content)} å­—ç¬¦")
                print(f"   LLMå“åº”å‰100å­—ç¬¦: {llm_interaction.response_content[:100]}...")
                if llm_interaction.usage_stats:
                    print(f"   Tokenä½¿ç”¨: {llm_interaction.usage_stats}")
        except Exception as llm_error:
            error_msg = f"LLMè°ƒç”¨å¼‚å¸¸: {str(llm_error)}"
            print(f"ğŸ’¥ [TASK-{task_id}] {error_msg}")
            print(f"ğŸ” [TASK-{task_id}] å¼‚å¸¸ç±»å‹: {type(llm_error).__name__}")
            import traceback
            traceback.print_exc()
            update_task_status(task_id, TaskStatus.FAILED, 0, error=error_msg)
            return
        
        # æ£€æŸ¥AIç”Ÿæˆæ˜¯å¦æˆåŠŸï¼ˆä¸´æ—¶é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ç”¨äºè°ƒè¯•ï¼‰
        print(f"ğŸ” [TASK-{task_id}] æ£€æŸ¥ç½®ä¿¡åº¦: {ai_result.confidence} (é˜ˆå€¼: 0.05)")
        if ai_result.confidence < 0.05:  # ä¸´æ—¶é™ä½é˜ˆå€¼ç”¨äºè°ƒè¯•
            error_msg = f"AIåˆ†æå¤±è´¥ï¼šç”Ÿæˆçš„ä»£ç è´¨é‡ä¸ç¬¦åˆè¦æ±‚ (ç½®ä¿¡åº¦: {ai_result.confidence:.2f})"
            print(f"âŒ [TASK-{task_id}] {error_msg}")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šå³ä½¿å¤±è´¥ä¹Ÿæ˜¾ç¤ºç”Ÿæˆçš„å†…å®¹
            print(f"ğŸ” [TASK-{task_id}] è°ƒè¯• - å¤±è´¥çš„ç”Ÿæˆç»“æœ:")
            print(f"   é¢˜ç›®ç±»å‹: {ai_result.problem_type}")
            print(f"   å‚æ•°: {ai_result.parameters}")
            print(f"   è¯´æ˜: {ai_result.explanation}")
            print(f"   ä»£ç å‰200å­—ç¬¦: {ai_result.visualization_code[:200]}...")
            
            update_task_status(task_id, TaskStatus.FAILED, 0, error=error_msg)
            return
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼šä»£ç éªŒè¯ä¸­
        update_task_status(task_id, TaskStatus.CODE_VALIDATING, 40, ai_analysis=ai_result)
        print(f"ğŸ“Š [TASK-{task_id}] çŠ¶æ€æ›´æ–°: CODE_VALIDATING (40%)")
        
        # 2. åœ¨æ²™ç®±ä¸­æ‰§è¡Œä»£ç 
        print(f"ğŸƒ [TASK-{task_id}] å¼€å§‹åœ¨æ²™ç®±ä¸­æ‰§è¡Œä»£ç ...")
        print(f"ğŸ”’ [TASK-{task_id}] æ‰§è¡Œæ¨¡å¼: {config_storage['default_execution_mode']}")
        
        sandbox_manager = get_sandbox_manager()
        sandbox_result = sandbox_manager.execute_code_safely(
            code=ai_result.visualization_code,
            output_path=f"output/{task_id}.png",
            execution_mode=config_storage["default_execution_mode"],
            timeout=30
        )
        
        print(f"ğŸ [TASK-{task_id}] æ²™ç®±æ‰§è¡Œå®Œæˆ!")
        print(f"ğŸ“Š [TASK-{task_id}] æ‰§è¡Œç»“æœ: {sandbox_result.get('overall_success', False)}")
        
        if not sandbox_result["overall_success"]:
            error_msg = f"ä»£ç æ‰§è¡Œå¤±è´¥: {sandbox_result['error_message']}"
            print(f"âŒ [TASK-{task_id}] {error_msg}")
            print(f"ğŸ” [TASK-{task_id}] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {sandbox_result}")
            update_task_status(task_id, TaskStatus.FAILED, 0, error=error_msg)
            return
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼šå®Œæˆ
        execution_result = sandbox_result["execution_result"]
        update_task_status(task_id, TaskStatus.COMPLETED, 100, 
                         ai_analysis=ai_result, execution_result=execution_result)
        
        print(f"ğŸ‰ [TASK-{task_id}] ä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ–¼ï¸  [TASK-{task_id}] å›¾ç‰‡è·¯å¾„: {execution_result.image_path}")
        print(f"â±ï¸  [TASK-{task_id}] æ‰§è¡Œæ—¶é—´: {execution_result.execution_time:.2f}ç§’")
        
    except Exception as e:
        error_msg = f"ä»»åŠ¡å¤„ç†å¼‚å¸¸: {str(e)}"
        print(f"ğŸ’¥ [TASK-{task_id}] {error_msg}")
        print(f"ğŸ” [TASK-{task_id}] å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {e}")
        import traceback
        print(f"ğŸ“š [TASK-{task_id}] å †æ ˆè·Ÿè¸ª:")
        traceback.print_exc()
        update_task_status(task_id, TaskStatus.FAILED, 0, error=error_msg)

def update_task_status(task_id: str, status: TaskStatus, progress: int,
                      ai_analysis: Optional[AIAnalysisResult] = None,
                      execution_result: Optional[ExecutionResult] = None,
                      error: Optional[str] = None,
                      llm_interaction: Optional[Dict[str, Any]] = None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    if task_id in task_storage:
        task_info = task_storage[task_id]
        task_info.status = status
        task_info.progress = progress
        task_info.updated_at = datetime.now().isoformat()
        
        if ai_analysis:
            task_info.ai_analysis = ai_analysis
        if execution_result:
            task_info.execution_result = execution_result
        if error:
            task_info.error_message = error
        if llm_interaction:
            from ..models.schema import LLMInteraction
            task_info.llm_interaction = LLMInteraction(**llm_interaction)

@router.post("/problems/generate", response_model=TaskResponse)
async def generate_visualization(request: ProblemRequest, background_tasks: BackgroundTasks):
    """
    ç”Ÿæˆæ•°å­¦é¢˜ç›®å¯è§†åŒ–ï¼ˆæ”¯æŒAIå’Œä¼ ç»Ÿæ¨¡å¼ï¼‰
    
    Args:
        request: é—®é¢˜è¯·æ±‚
        background_tasks: åå°ä»»åŠ¡ç®¡ç†å™¨
        
    Returns:
        TaskResponse: ä»»åŠ¡å“åº”
    """
    try:
        print(f"ğŸ” [API] æ”¶åˆ°è¯·æ±‚: {request}")
        
        # æ£€æŸ¥å¹¶å‘ä»»åŠ¡æ•°é‡
        active_tasks = sum(1 for task in task_storage.values() 
                          if task.status in [TaskStatus.PENDING, TaskStatus.AI_ANALYZING, 
                                           TaskStatus.CODE_VALIDATING, TaskStatus.EXECUTING])
        
        if active_tasks >= config_storage["max_concurrent_tasks"]:
            print(f"âŒ [API] ä»»åŠ¡æ•°è¶…é™: {active_tasks}/{config_storage['max_concurrent_tasks']}")
            raise HTTPException(status_code=429, detail="æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•")
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task_info = TaskInfo(
            task_id=task_id,
            status=TaskStatus.PENDING,
            progress=0,
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat(),
            processing_mode=request.processing_mode,
            llm_provider=request.llm_provider if request.processing_mode == ProcessingMode.AI else None
        )
        
        task_storage[task_id] = task_info
        print(f"âœ… [API] åˆ›å»ºä»»åŠ¡: {task_id}, æ¨¡å¼: {request.processing_mode}")
        
        # æ·»åŠ åå°ä»»åŠ¡
        if request.processing_mode == ProcessingMode.AI:
            # æ£€æŸ¥APIå¯†é’¥
            print(f"ğŸ”‘ [API] æ£€æŸ¥APIå¯†é’¥é…ç½®, æä¾›å•†: {request.llm_provider}")
            llm_manager = get_llm_manager()
            available_providers = llm_manager.get_available_providers()
            print(f"ğŸ” [API] å¯ç”¨æä¾›å•†: {available_providers}")
            
            if request.llm_provider not in available_providers:
                error_msg = f"æœªé…ç½®{request.llm_provider.value}çš„APIå¯†é’¥"
                print(f"âŒ [API] {error_msg}")
                raise HTTPException(status_code=400, detail=error_msg)
            
            background_tasks.add_task(process_ai_visualization_task, task_id, request)
            estimated_time = 15
        else:
            # ä¼ ç»Ÿæ¨¡å¼å¤„ç†ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            from text_to_visual import MathProblemVisualizer
            
            async def process_traditional_task():
                try:
                    update_task_status(task_id, TaskStatus.EXECUTING, 50)
                    
                    visualizer = MathProblemVisualizer()
                    if "ç›¸é‡" in request.text:
                        img_path, result = visualizer.create_meeting_visualization(
                            request.text, f"output/{task_id}.png"
                        )
                    elif "è¿½åŠ" in request.text:
                        img_path, result = visualizer.create_chase_visualization(
                            request.text, f"output/{task_id}.png"
                        )
                    else:
                        # é»˜è®¤ä½¿ç”¨ç›¸é‡é—®é¢˜å¤„ç†
                        img_path, result = visualizer.create_meeting_visualization(
                            request.text, f"output/{task_id}.png"
                        )
                    
                    execution_result = ExecutionResult(
                        success=True,
                        image_path=img_path,
                        result_data=result,
                        execution_time=2.0,
                        memory_usage=50.0,
                        output_logs="ä¼ ç»Ÿæ¨¡å¼å¤„ç†å®Œæˆ"
                    )
                    
                    update_task_status(task_id, TaskStatus.COMPLETED, 100, 
                                     execution_result=execution_result)
                    
                except Exception as e:
                    update_task_status(task_id, TaskStatus.FAILED, 0, 
                                     error=f"ä¼ ç»Ÿæ¨¡å¼å¤„ç†å¤±è´¥: {str(e)}")
            
            background_tasks.add_task(process_traditional_task)
            estimated_time = 5
        
        print(f"âœ… [API] ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}")
        return TaskResponse(
            task_id=task_id,
            status=TaskStatus.PENDING,
            message="ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨å¤„ç†ä¸­...",
            estimated_time=estimated_time
        )
        
    except HTTPException as e:
        print(f"âŒ [API] HTTPå¼‚å¸¸: {e.status_code} - {e.detail}")
        raise
    except Exception as e:
        print(f"ğŸ’¥ [API] æœªé¢„æœŸå¼‚å¸¸: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")

@router.get("/tasks/{task_id}", response_model=TaskInfo)
async def get_task_status(task_id: str):
    """
    è·å–ä»»åŠ¡çŠ¶æ€
    
    Args:
        task_id: ä»»åŠ¡ID
        
    Returns:
        TaskInfo: ä»»åŠ¡ä¿¡æ¯
    """
    if task_id not in task_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return task_storage[task_id]

@router.get("/tasks")
async def list_tasks(limit: int = 10, status: Optional[TaskStatus] = None):
    """
    åˆ—å‡ºä»»åŠ¡
    
    Args:
        limit: è¿”å›æ•°é‡é™åˆ¶
        status: çŠ¶æ€è¿‡æ»¤
        
    Returns:
        list: ä»»åŠ¡åˆ—è¡¨
    """
    tasks = list(task_storage.values())
    
    if status:
        tasks = [task for task in tasks if task.status == status]
    
    # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    tasks.sort(key=lambda x: x.created_at, reverse=True)
    
    return tasks[:limit]

@router.delete("/tasks/{task_id}")
async def delete_task(task_id: str):
    """
    åˆ é™¤ä»»åŠ¡
    
    Args:
        task_id: ä»»åŠ¡ID
    """
    if task_id not in task_storage:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    del task_storage[task_id]
    return {"message": "ä»»åŠ¡å·²åˆ é™¤"}

@router.post("/config/api-key")
async def set_api_key(provider: LLMProvider, api_key: str):
    """
    è®¾ç½®APIå¯†é’¥
    
    Args:
        provider: æ¨¡å‹æä¾›å•†
        api_key: APIå¯†é’¥
    """
    # æ›´æ–°é…ç½®ç®¡ç†å™¨
    config_manager = get_config_manager()
    config_manager.set_api_key(provider, api_key)
    
    # æ›´æ–°LLMç®¡ç†å™¨
    llm_manager = get_llm_manager()
    llm_manager.set_api_key(provider, api_key)
    
    return {"message": f"{provider.value}çš„APIå¯†é’¥å·²è®¾ç½®"}

@router.post("/config/base-url")
async def set_base_url(provider: LLMProvider, base_url: str):
    """
    è®¾ç½®Base URL
    
    Args:
        provider: æ¨¡å‹æä¾›å•†
        base_url: Base URL
    """
    # æ›´æ–°é…ç½®ç®¡ç†å™¨
    config_manager = get_config_manager()
    config_manager.set_base_url(provider, base_url)
    
    return {"message": f"{provider.value}çš„Base URLå·²è®¾ç½®ä¸º: {base_url}"}

@router.get("/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    config_manager = get_config_manager()
    llm_manager = get_llm_manager()
    
    return {
        "configured_providers": [p.value for p in config_manager.get_configured_providers()],
        "default_provider": config_manager.get_default_provider().value,
        "default_execution_mode": config_storage["default_execution_mode"],
        "max_concurrent_tasks": config_storage["max_concurrent_tasks"],
        "config_summary": config_manager.get_config_summary()
    }

@router.post("/config")
async def update_config(
    default_provider: Optional[LLMProvider] = None,
    default_execution_mode: Optional[str] = None,
    max_concurrent_tasks: Optional[int] = None
):
    """æ›´æ–°é…ç½®"""
    config_manager = get_config_manager()
    
    if default_provider:
        config_manager.default_provider = default_provider
    if default_execution_mode:
        config_storage["default_execution_mode"] = default_execution_mode
    if max_concurrent_tasks:
        config_storage["max_concurrent_tasks"] = max_concurrent_tasks
    
    return {"message": "é…ç½®å·²æ›´æ–°"}

@router.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    # ä»»åŠ¡ç»Ÿè®¡
    task_stats = {
        "total_tasks": len(task_storage),
        "pending_tasks": sum(1 for task in task_storage.values() if task.status == TaskStatus.PENDING),
        "processing_tasks": sum(1 for task in task_storage.values() 
                              if task.status in [TaskStatus.AI_ANALYZING, TaskStatus.CODE_VALIDATING, TaskStatus.EXECUTING]),
        "completed_tasks": sum(1 for task in task_storage.values() if task.status == TaskStatus.COMPLETED),
        "failed_tasks": sum(1 for task in task_storage.values() if task.status == TaskStatus.FAILED)
    }
    
    # AIç”Ÿæˆç»Ÿè®¡
    code_generator = get_code_generator()
    ai_stats = code_generator.get_generation_stats()
    
    # æ²™ç®±ç»Ÿè®¡
    sandbox_manager = get_sandbox_manager()
    sandbox_stats = sandbox_manager.get_sandbox_stats()
    
    return {
        "task_stats": task_stats,
        "ai_generation_stats": ai_stats,
        "sandbox_stats": sandbox_stats,
        "system_config": {
            "configured_providers": len(config_storage["api_keys"]),
            "max_concurrent_tasks": config_storage["max_concurrent_tasks"]
        }
    }

@router.get("/health", response_model=HealthStatus)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    active_tasks = sum(1 for task in task_storage.values() 
                      if task.status in [TaskStatus.PENDING, TaskStatus.AI_ANALYZING, 
                                       TaskStatus.CODE_VALIDATING, TaskStatus.EXECUTING])
    
    total_processed = len([task for task in task_storage.values() 
                          if task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]])
    
    # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
    completed_tasks = [task for task in task_storage.values() if task.status == TaskStatus.COMPLETED]
    avg_time = 0.0
    if completed_tasks:
        total_time = 0.0
        count = 0
        for task in completed_tasks:
            if task.ai_analysis:
                total_time += task.ai_analysis.processing_time
                count += 1
            if task.execution_result:
                total_time += task.execution_result.execution_time
                count += 1
        avg_time = total_time / count if count > 0 else 0.0
    
    return HealthStatus(
        status="healthy",
        timestamp=datetime.now().isoformat(),
        version="2.0.0",
        active_tasks=active_tasks,
        total_processed=total_processed,
        avg_processing_time=avg_time
    )

@router.post("/test/connection/{provider}")
async def test_llm_connection(provider: LLMProvider):
    """æµ‹è¯•LLMè¿æ¥"""
    llm_manager = get_llm_manager()
    
    if provider not in llm_manager.get_available_providers():
        raise HTTPException(status_code=400, detail=f"æœªé…ç½®{provider.value}çš„APIå¯†é’¥")
    
    try:
        success = await llm_manager.test_connection(provider)
        return {"success": success, "message": f"{provider.value}è¿æ¥{'æˆåŠŸ' if success else 'å¤±è´¥'}"}
    except Exception as e:
        return {"success": False, "message": f"è¿æ¥æµ‹è¯•å¼‚å¸¸: {str(e)}"}

# å¯¼å‡ºè·¯ç”±å™¨
__all__ = ['router']
